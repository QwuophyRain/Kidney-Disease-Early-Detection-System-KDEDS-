{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QwuophyRain/Kidney-Disease-Early-Detection-System-KDEDS-/blob/main/KDEDS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kidney Disease Early Detection System (KDEDS)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IM8FFpGjg5Kn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Bidirectional, Attention, Dropout, MultiHeadAttention\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "import datetime\n",
        "\n",
        "class KidneyDiseaseEarlyDetectionSystem:\n",
        "    \"\"\"\n",
        "    A comprehensive system for early detection of kidney diseases using\n",
        "    temporal deep learning with attention mechanisms.\n",
        "    \"\"\"\n",
        "\n",
        "    def _init_(self, config=None):\n",
        "        \"\"\"\n",
        "        Initialize the kidney disease early detection system.\n",
        "\n",
        "        Args:\n",
        "            config: Dictionary containing configuration parameters\n",
        "        \"\"\"\n",
        "        self.config = {\n",
        "            'sequence_length': 10,  # Number of time steps to consider\n",
        "            'feature_dim': 42,      # Number of features after processing\n",
        "            'lstm_units': 128,      # Number of LSTM units\n",
        "            'attention_heads': 8,   # Number of attention heads\n",
        "            'dropout_rate': 0.3,    # Dropout rate for regularization\n",
        "            'learning_rate': 0.001, # Learning rate for optimizer\n",
        "            'batch_size': 64,       # Batch size for training\n",
        "            'epochs': 100,          # Maximum epochs for training\n",
        "            'early_stopping': 10,   # Patience for early stopping\n",
        "            'threshold': 0.7,       # Default prediction threshold\n",
        "        }\n",
        "\n",
        "        if config:\n",
        "            self.config.update(config)\n",
        "\n",
        "        self.model = None\n",
        "        self.data_preprocessor = None\n",
        "        self.feature_names = None\n",
        "        self.explainer = None\n",
        "\n",
        "    def preprocess_data(self, data, is_training=True):\n",
        "        \"\"\"\n",
        "        Preprocess the input data for the model.\n",
        "\n",
        "        Args:\n",
        "            data: DataFrame containing patient data\n",
        "            is_training: Whether this is for training or inference\n",
        "\n",
        "        Returns:\n",
        "            Preprocessed data ready for the model\n",
        "        \"\"\"\n",
        "        # Handle missing values using Multiple Imputation by Chained Equations\n",
        "        if is_training:\n",
        "            self.data_preprocessor = IterativeImputer(max_iter=10, random_state=42)\n",
        "            imputed_data = self.data_preprocessor.fit_transform(data.select_dtypes(include=[np.number]))\n",
        "            joblib.dump(self.data_preprocessor, 'kidney_imputer.pkl')\n",
        "        else:\n",
        "            if self.data_preprocessor is None:\n",
        "                self.data_preprocessor = joblib.load('kidney_imputer.pkl')\n",
        "            imputed_data = self.data_preprocessor.transform(data.select_dtypes(include=[np.number]))\n",
        "\n",
        "        # Replace the numeric columns with imputed values\n",
        "        numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
        "        data_imputed = data.copy()\n",
        "        data_imputed[numeric_cols] = imputed_data\n",
        "\n",
        "        # Create temporal sequences for each patient\n",
        "        patient_ids = data_imputed['patient_id'].unique()\n",
        "        sequences = []\n",
        "        labels = []\n",
        "\n",
        "        for patient_id in patient_ids:\n",
        "            patient_data = data_imputed[data_imputed['patient_id'] == patient_id].sort_values('timestamp')\n",
        "\n",
        "            # Extract features and normalize\n",
        "            features = patient_data.drop(['patient_id', 'timestamp', 'kidney_disease'], axis=1)\n",
        "            if is_training:\n",
        "                self.feature_names = features.columns.tolist()\n",
        "                self.scaler = StandardScaler()\n",
        "                features_scaled = self.scaler.fit_transform(features)\n",
        "                joblib.dump(self.scaler, 'kidney_scaler.pkl')\n",
        "            else:\n",
        "                if self.scaler is None:\n",
        "                    self.scaler = joblib.load('kidney_scaler.pkl')\n",
        "                features_scaled = self.scaler.transform(features)\n",
        "\n",
        "            # Create sequences of specified length\n",
        "            for i in range(len(patient_data) - self.config['sequence_length'] + 1):\n",
        "                seq = features_scaled[i:i + self.config['sequence_length']]\n",
        "                sequences.append(seq)\n",
        "\n",
        "                # Label is positive if kidney disease is diagnosed within next 6 months\n",
        "                future_diagnosis = patient_data.iloc[i + self.config['sequence_length'] - 1:]['kidney_disease'].max()\n",
        "                labels.append(future_diagnosis)\n",
        "\n",
        "        return np.array(sequences), np.array(labels)\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"\n",
        "        Build the multi-stage temporal deep learning model with attention mechanisms.\n",
        "\n",
        "        Returns:\n",
        "            Compiled Keras model\n",
        "        \"\"\"\n",
        "        # Input layer\n",
        "        input_layer = Input(shape=(self.config['sequence_length'], self.config['feature_dim']))\n",
        "\n",
        "        # Bidirectional LSTM to capture temporal patterns\n",
        "        lstm_layer = Bidirectional(LSTM(self.config['lstm_units'], return_sequences=True))(input_layer)\n",
        "        lstm_layer = Dropout(self.config['dropout_rate'])(lstm_layer)\n",
        "\n",
        "        # Multi-head attention mechanism\n",
        "        attention_layer = MultiHeadAttention(\n",
        "            num_heads=self.config['attention_heads'],\n",
        "            key_dim=self.config['lstm_units'] // self.config['attention_heads']\n",
        "        )(lstm_layer, lstm_layer)\n",
        "\n",
        "        # Skip connection\n",
        "        combined = tf.keras.layers.Add()([lstm_layer, attention_layer])\n",
        "        combined = tf.keras.layers.LayerNormalization()(combined)\n",
        "\n",
        "        # Flatten and dense layers for classification\n",
        "        flatten = tf.keras.layers.Flatten()(combined)\n",
        "        dense1 = Dense(64, activation='relu')(flatten)\n",
        "        dense1 = Dropout(self.config['dropout_rate'])(dense1)\n",
        "        output = Dense(1, activation='sigmoid')(dense1)\n",
        "\n",
        "        # Create and compile model\n",
        "        model = Model(inputs=input_layer, outputs=output)\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=self.config['learning_rate']),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
        "        )\n",
        "\n",
        "        self.model = model\n",
        "        return model\n",
        "\n",
        "    def train(self, train_data, validation_data=None, validation_split=0.2):\n",
        "        \"\"\"\n",
        "        Train the model on the provided data.\n",
        "\n",
        "        Args:\n",
        "            train_data: DataFrame containing training data\n",
        "            validation_data: Optional DataFrame for validation\n",
        "            validation_split: Validation split if validation_data not provided\n",
        "\n",
        "        Returns:\n",
        "            Training history\n",
        "        \"\"\"\n",
        "        # Preprocess the data\n",
        "        X_train, y_train = self.preprocess_data(train_data, is_training=True)\n",
        "\n",
        "        # Split for validation if validation_data not provided\n",
        "        if validation_data is not None:\n",
        "            X_val, y_val = self.preprocess_data(validation_data, is_training=False)\n",
        "        else:\n",
        "            X_train, X_val, y_train, y_val = train_test_split(\n",
        "                X_train, y_train, test_size=validation_split, random_state=42\n",
        "            )\n",
        "\n",
        "        # Build the model if not already built\n",
        "        if self.model is None:\n",
        "            self.build_model()\n",
        "\n",
        "        # Set up callbacks\n",
        "        early_stopping = EarlyStopping(\n",
        "            monitor='val_auc',\n",
        "            patience=self.config['early_stopping'],\n",
        "            mode='max',\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        history = self.model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            epochs=self.config['epochs'],\n",
        "            batch_size=self.config['batch_size'],\n",
        "            callbacks=[early_stopping],\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Build explainer for model interpretability\n",
        "        self._build_explainer(X_train)\n",
        "\n",
        "        return history\n",
        "\n",
        "    def _build_explainer(self, train_data_sample):\n",
        "        \"\"\"\n",
        "        Build a SHAP explainer for model interpretability.\n",
        "\n",
        "        Args:\n",
        "            train_data_sample: Sample of training data for explainer\n",
        "        \"\"\"\n",
        "        # Create a simplified model to explain predictions\n",
        "        def model_predict(x):\n",
        "            return self.model.predict(x)\n",
        "\n",
        "        # Create the explainer\n",
        "        self.explainer = shap.KernelExplainer(\n",
        "            model_predict,\n",
        "            shap.sample(train_data_sample, 100)  # Sample 100 instances for background distribution\n",
        "        )\n",
        "\n",
        "    def predict(self, patient_data):\n",
        "        \"\"\"\n",
        "        Generate predictions for patient data.\n",
        "\n",
        "        Args:\n",
        "            patient_data: DataFrame containing patient data\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing predictions and explanations\n",
        "        \"\"\"\n",
        "        # Preprocess the data\n",
        "        X_test, _ = self.preprocess_data(patient_data, is_training=False)\n",
        "\n",
        "        # Generate predictions\n",
        "        risk_scores = self.model.predict(X_test).flatten()\n",
        "        predictions = risk_scores > self.config['threshold']\n",
        "\n",
        "        # Generate explanations for predictions\n",
        "        if self.explainer is not None:\n",
        "            try:\n",
        "                shap_values = self.explainer.shap_values(X_test[:10])  # Limit to first 10 for efficiency\n",
        "                feature_importance = {}\n",
        "                for i, feature in enumerate(self.feature_names):\n",
        "                    feature_importance[feature] = np.abs(shap_values[0][:, i]).mean()\n",
        "            except:\n",
        "                feature_importance = None\n",
        "        else:\n",
        "            feature_importance = None\n",
        "\n",
        "        # Return results\n",
        "        results = {\n",
        "            'patient_ids': patient_data['patient_id'].unique().tolist(),\n",
        "            'risk_scores': risk_scores.tolist(),\n",
        "            'predictions': predictions.tolist(),\n",
        "            'feature_importance': feature_importance,\n",
        "            'timestamp': datetime.datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def save_model(self, path='kidney_model'):\n",
        "        \"\"\"\n",
        "        Save the trained model and associated components.\n",
        "\n",
        "        Args:\n",
        "            path: Base path for saving\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model has not been trained yet\")\n",
        "\n",
        "        # Save Keras model\n",
        "        self.model.save(f'{path}.h5')\n",
        "\n",
        "        # Save configuration and preprocessing components\n",
        "        np.save(f'{path}_config.npy', self.config)\n",
        "        if self.feature_names:\n",
        "            with open(f'{path}_features.txt', 'w') as f:\n",
        "                f.write('\\n'.join(self.feature_names))\n",
        "\n",
        "    def load_model(self, path='kidney_model'):\n",
        "        \"\"\"\n",
        "        Load a trained model and associated components.\n",
        "\n",
        "        Args:\n",
        "            path: Base path for loading\n",
        "        \"\"\"\n",
        "        # Load Keras model\n",
        "        self.model = tf.keras.models.load_model(f'{path}.h5')\n",
        "\n",
        "        # Load configuration and preprocessing components\n",
        "        self.config = np.load(f'{path}_config.npy', allow_pickle=True).item()\n",
        "        with open(f'{path}_features.txt', 'r') as f:\n",
        "            self.feature_names = [line.strip() for line in f.readlines()]\n",
        "\n",
        "        self.scaler = joblib.load('kidney_scaler.pkl')\n",
        "        self.data_preprocessor = joblib.load('kidney_imputer.pkl')\n",
        "\n",
        "    def evaluate(self, test_data, threshold=None):\n",
        "        \"\"\"\n",
        "        Evaluate the model performance on test data.\n",
        "\n",
        "        Args:\n",
        "            test_data: DataFrame containing test data\n",
        "            threshold: Optional custom threshold for positive prediction\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing evaluation metrics\n",
        "        \"\"\"\n",
        "        # Use default threshold if not specified\n",
        "        if threshold is None:\n",
        "            threshold = self.config['threshold']\n",
        "\n",
        "        # Preprocess the data\n",
        "        X_test, y_test = self.preprocess_data(test_data, is_training=False)\n",
        "\n",
        "        # Generate predictions\n",
        "        y_pred_proba = self.model.predict(X_test).flatten()\n",
        "        y_pred = y_pred_proba > threshold\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = np.mean(y_pred == y_test)\n",
        "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "        # Calculate precision-recall curve and AUC\n",
        "        precision, recall, pr_thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
        "        pr_auc = auc(recall, precision)\n",
        "\n",
        "        # Find the threshold that maximizes F1 score\n",
        "        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
        "        best_threshold_idx = np.argmax(f1_scores)\n",
        "        best_threshold = pr_thresholds[best_threshold_idx]\n",
        "\n",
        "        # Calculate confusion matrix\n",
        "        TP = np.sum((y_pred == 1) & (y_test == 1))\n",
        "        TN = np.sum((y_pred == 0) & (y_test == 0))\n",
        "        FP = np.sum((y_pred == 1) & (y_test == 0))\n",
        "        FN = np.sum((y_pred == 0) & (y_test == 1))\n",
        "\n",
        "        # Compile evaluation results\n",
        "        results = {\n",
        "            'accuracy': accuracy,\n",
        "            'roc_auc': roc_auc,\n",
        "            'pr_auc': pr_auc,\n",
        "            'sensitivity': TP / (TP + FN) if (TP + FN) > 0 else 0,\n",
        "            'specificity': TN / (TN + FP) if (TN + FP) > 0 else 0,\n",
        "            'precision': TP / (TP + FP) if (TP + FP) > 0 else 0,\n",
        "            'recall': TP / (TP + FN) if (TP + FN) > 0 else 0,\n",
        "            'f1_score': 2 * TP / (2 * TP + FP + FN) if (2 * TP + FP + FN) > 0 else 0,\n",
        "            'best_threshold': best_threshold,\n",
        "            'confusion_matrix': {\n",
        "                'TP': int(TP),\n",
        "                'TN': int(TN),\n",
        "                'FP': int(FP),\n",
        "                'FN': int(FN)\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def visualize_predictions(self, patient_data, save_path=None):\n",
        "        \"\"\"\n",
        "        Visualize predictions and feature importance for a patient.\n",
        "\n",
        "        Args:\n",
        "            patient_data: DataFrame containing data for a single patient\n",
        "            save_path: Optional path to save visualization\n",
        "\n",
        "        Returns:\n",
        "            Matplotlib figure\n",
        "        \"\"\"\n",
        "        # Ensure we're working with a single patient\n",
        "        patient_id = patient_data['patient_id'].iloc[0]\n",
        "        patient_data_sorted = patient_data.sort_values('timestamp')\n",
        "\n",
        "        # Preprocess and predict\n",
        "        X, _ = self.preprocess_data(patient_data, is_training=False)\n",
        "        risk_scores = self.model.predict(X).flatten()\n",
        "\n",
        "        # Calculate SHAP values\n",
        "        if self.explainer is not None:\n",
        "            shap_values = self.explainer.shap_values(X[0:1])[0]\n",
        "            feature_importance = {}\n",
        "            for i, feature in enumerate(self.feature_names):\n",
        "                feature_importance[feature] = np.abs(shap_values[:, i]).mean()\n",
        "\n",
        "            # Sort features by importance\n",
        "            sorted_features = sorted(\n",
        "                feature_importance.items(),\n",
        "                key=lambda x: x[1],\n",
        "                reverse=True\n",
        "            )[:10]  # Top 10 features\n",
        "        else:\n",
        "            sorted_features = []\n",
        "\n",
        "        # Create visualization\n",
        "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), gridspec_kw={'height_ratios': [1, 2]})\n",
        "\n",
        "        # Plot risk trajectory\n",
        "        timestamps = patient_data_sorted['timestamp'].iloc[self.config['sequence_length']-1:].values\n",
        "        ax1.plot(timestamps, risk_scores, 'o-', color='blue')\n",
        "        ax1.axhline(y=self.config['threshold'], color='red', linestyle='--', label='Risk Threshold')\n",
        "        ax1.set_title(f'Kidney Disease Risk Trajectory for Patient {patient_id}')\n",
        "        ax1.set_ylabel('Risk Score')\n",
        "        ax1.set_ylim(0, 1)\n",
        "        ax1.legend()\n",
        "        ax1.grid(True)\n",
        "\n",
        "        # Plot feature importance\n",
        "        if sorted_features:\n",
        "            features, importance = zip(*sorted_features)\n",
        "            y_pos = np.arange(len(features))\n",
        "            ax2.barh(y_pos, importance, align='center')\n",
        "            ax2.set_yticks(y_pos)\n",
        "            ax2.set_yticklabels(features)\n",
        "            ax2.invert_yaxis()  # Labels read top-to-bottom\n",
        "            ax2.set_title('Top 10 Features Influencing Prediction')\n",
        "            ax2.set_xlabel('Feature Importance (SHAP value)')\n",
        "        else:\n",
        "            ax2.text(0.5, 0.5, 'Feature importance not available',\n",
        "                    horizontalalignment='center', verticalalignment='center',\n",
        "                    transform=ax2.transAxes)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(save_path)\n",
        "\n",
        "        return fig\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if _name_ == \"_main_\":\n",
        "    # Load sample data (this would be real patient data in practice)\n",
        "    # This is just a placeholder - in reality, you would load your own dataset\n",
        "    from sklearn.datasets import make_classification\n",
        "    import random\n",
        "\n",
        "    # Create synthetic patient data for demonstration\n",
        "    def create_synthetic_patient_data(n_patients=100, n_visits_per_patient=15):\n",
        "        \"\"\"Create synthetic patient data for demonstration purposes.\"\"\"\n",
        "        data_rows = []\n",
        "\n",
        "        # Features that would be relevant for kidney disease prediction\n",
        "        features = [\n",
        "            'age', 'gender', 'weight', 'height', 'blood_pressure_systolic',\n",
        "            'blood_pressure_diastolic', 'heart_rate', 'temperature',\n",
        "            'serum_creatinine', 'blood_urea_nitrogen', 'glomerular_filtration_rate',\n",
        "            'urine_albumin', 'urine_creatinine', 'albumin_creatinine_ratio',\n",
        "            'serum_sodium', 'serum_potassium', 'serum_chloride', 'serum_bicarbonate',\n",
        "            'hemoglobin', 'diabetes', 'hypertension', 'cardiovascular_disease',\n",
        "            'smoking_status', 'alcohol_consumption', 'medication_ace_inhibitors',\n",
        "            'medication_arbs', 'medication_diuretics', 'medication_beta_blockers',\n",
        "            'medication_nsaids', 'family_history_kidney_disease'\n",
        "        ]\n",
        "\n",
        "        # Generate base values for each patient\n",
        "        patient_base_values = {}\n",
        "        for i in range(n_patients):\n",
        "            patient_id = f\"P{i:04d}\"\n",
        "\n",
        "            # Generate base values that will stay relatively consistent for each patient\n",
        "            patient_base_values[patient_id] = {\n",
        "                'age': random.randint(18, 85),\n",
        "                'gender': random.choice([0, 1]),  # 0 for female, 1 for male\n",
        "                'weight': random.uniform(50, 120),\n",
        "                'height': random.uniform(150, 190),\n",
        "                'diabetes': random.choice([0, 0, 0, 1]),  # 25% chance of diabetes\n",
        "                'hypertension': random.choice([0, 0, 1]),  # 33% chance of hypertension\n",
        "                'cardiovascular_disease': random.choice([0, 0, 0, 1]),  # 25% chance of CVD\n",
        "                'smoking_status': random.choice([0, 0, 1]),  # 33% chance of smoking\n",
        "                'alcohol_consumption': random.choice([0, 1, 2]),  # 0-none, 1-moderate, 2-heavy\n",
        "                'family_history_kidney_disease': random.choice([0, 0, 0, 1])  # 25% chance\n",
        "            }\n",
        "\n",
        "            # Determine if this patient will develop kidney disease\n",
        "            # Higher risk for patients with diabetes, hypertension, and older age\n",
        "            risk_factors = (\n",
        "                patient_base_values[patient_id]['diabetes'] +\n",
        "                patient_base_values[patient_id]['hypertension'] +\n",
        "                (1 if patient_base_values[patient_id]['age'] > 60 else 0) +\n",
        "                patient_base_values[patient_id]['cardiovascular_disease'] +\n",
        "                patient_base_values[patient_id]['family_history_kidney_disease']\n",
        "            )\n",
        "\n",
        "            will_develop_kidney_disease = random.random() < (0.1 + 0.15 * risk_factors)\n",
        "\n",
        "            # When the disease will be diagnosed (if at all)\n",
        "            diagnosis_visit = random.randint(n_visits_per_patient // 2, n_visits_per_patient) if will_develop_kidney_disease else None\n",
        "\n",
        "            # Generate visits for this patient\n",
        "            for visit in range(n_visits_per_patient):\n",
        "                timestamp = f\"2023-{random.randint(1, 12):02d}-{random.randint(1, 28):02d}\"\n",
        "\n",
        "                # Create a row for this visit\n",
        "                row = {'patient_id': patient_id, 'timestamp': timestamp}\n",
        "\n",
        "                # Add base values with small random variations\n",
        "                for feature, base_value in patient_base_values[patient_id].items():\n",
        "                    if isinstance(base_value, int) and base_value in [0, 1]:  # Binary features\n",
        "                        row[feature] = base_value\n",
        "                    elif feature == 'age':\n",
        "                        # Age increases slightly over time\n",
        "                        row[feature] = base_value + visit / 12  # Assuming monthly visits\n",
        "                    else:\n",
        "                        # Add small random variations to continuous features\n",
        "                        row[feature] = base_value * random.uniform(0.95, 1.05)\n",
        "\n",
        "                # Add visit-specific measurements\n",
        "                # Normal ranges for kidney-related biomarkers\n",
        "                row['serum_creatinine'] = random.uniform(0.6, 1.2)  # mg/dL\n",
        "                row['blood_urea_nitrogen'] = random.uniform(7, 20)  # mg/dL\n",
        "                row['glomerular_filtration_rate'] = random.uniform(90, 120)  # mL/min/1.73m²\n",
        "                row['urine_albumin'] = random.uniform(0, 30)  # mg/24h\n",
        "                row['urine_creatinine'] = random.uniform(800, 2000)  # mg/24h\n",
        "                row['albumin_creatinine_ratio'] = row['urine_albumin'] / row['urine_creatinine'] * 1000  # mg/g\n",
        "\n",
        "                # Other biomarkers\n",
        "                row['serum_sodium'] = random.uniform(135, 145)  # mEq/L\n",
        "                row['serum_potassium'] = random.uniform(3.5, 5.0)  # mEq/L\n",
        "                row['serum_chloride'] = random.uniform(96, 106)  # mEq/L\n",
        "                row['serum_bicarbonate'] = random.uniform(22, 29)  # mEq/L\n",
        "                row['hemoglobin'] = random.uniform(12, 17)  # g/dL\n",
        "\n",
        "                # Vital signs\n",
        "                row['blood_pressure_systolic'] = random.uniform(110, 140)\n",
        "                row['blood_pressure_diastolic'] = random.uniform(70, 90)\n",
        "                row['heart_rate'] = random.uniform(60, 100)\n",
        "                row['temperature'] = random.uniform(36.1, 37.2)\n",
        "\n",
        "                # Medications\n",
        "                row['medication_ace_inhibitors'] = random.choice([0, 0, 1]) if row['hypertension'] else 0\n",
        "                row['medication_arbs'] = random.choice([0, 0, 1]) if row['hypertension'] else 0\n",
        "                row['medication_diuretics'] = random.choice([0, 0, 1]) if row['hypertension'] else 0\n",
        "                row['medication_beta_blockers'] = random.choice([0, 0, 1]) if row['cardiovascular_disease'] else 0\n",
        "                row['medication_nsaids'] = random.choice([0, 0, 0, 1])  # 25% chance regardless\n",
        "\n",
        "                # For patients who will develop kidney disease, gradually worsen biomarkers\n",
        "                if will_develop_kidney_disease and visit >= diagnosis_visit // 2:\n",
        "                    progress_factor = (visit - diagnosis_visit // 2) / (diagnosis_visit - diagnosis_visit // 2) if visit < diagnosis_visit else 1\n",
        "\n",
        "                    # Worsen kidney function markers\n",
        "                    row['serum_creatinine'] += progress_factor * random.uniform(0.2, 1.5)\n",
        "                    row['blood_urea_nitrogen'] += progress_factor * random.uniform(5, 40)\n",
        "                    row['glomerular_filtration_rate'] -= progress_factor * random.uniform(10, 60)\n",
        "                    row['urine_albumin'] += progress_factor * random.uniform(30, 300)\n",
        "                    row['albumin_creatinine_ratio'] += progress_factor * random.uniform(30, 300)\n",
        "\n",
        "                    # Also affect other markers\n",
        "                    row['serum_potassium'] += progress_factor * random.uniform(0, 1.5)\n",
        "                    row['hemoglobin'] -= progress_factor * random.uniform(0, 3)\n",
        "\n",
        "                # Set the kidney_disease flag\n",
        "                row['kidney_disease'] = 1 if (will_develop_kidney_disease and visit >= diagnosis_visit) else 0\n",
        "\n",
        "                # Add the row to our dataset\n",
        "                data_rows.append(row)\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        df = pd.DataFrame(data_rows)\n",
        "\n",
        "        # Ensure all needed columns exist\n",
        "        for feature in features:\n",
        "            if feature not in df.columns:\n",
        "                df[feature] = 0\n",
        "\n",
        "        return df\n",
        "\n",
        "    # Create synthetic dataset\n",
        "    synthetic_data = create_synthetic_patient_data(n_patients=200, n_visits_per_patient=20)\n",
        "\n",
        "    # Split into train/test sets by patient\n",
        "    patient_ids = synthetic_data['patient_id'].unique()\n",
        "    train_patients, test_patients = train_test_split(patient_ids, test_size=0.2, random_state=42)\n",
        "\n",
        "    train_data = synthetic_data[synthetic_data['patient_id'].isin(train_patients)]\n",
        "    test_data = synthetic_data[synthetic_data['patient_id'].isin(test_patients)]\n",
        "\n",
        "    # Initialize and train the model\n",
        "    kdeds = KidneyDiseaseEarlyDetectionSystem()\n",
        "    history = kdeds.train(train_data)\n",
        "\n",
        "    # Evaluate on test data\n",
        "    evaluation = kdeds.evaluate(test_data)\n",
        "    print(\"Model Evaluation:\")\n",
        "    for metric, value in evaluation.items():\n",
        "        if metric != 'confusion_matrix':\n",
        "            print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "    # Get predictions for a sample patient\n",
        "    sample_patient_id = test_patients[0]\n",
        "    sample_patient_data = test_data[test_data['patient_id'] == sample_patient_id]\n",
        "    predictions = kdeds.predict(sample_patient_data)\n",
        "\n",
        "    # Visualize predictions\n",
        "    kdeds.visualize_predictions(sample_patient_data, save_path=\"kidney_prediction_visualization.png\")\n",
        "\n",
        "    # Save the model\n",
        "    kdeds.save_model()\n",
        "\n",
        "    print(\"Example completed. Model trained, evaluated, and saved.\")"
      ],
      "metadata": {
        "id": "-PPn25JRjN4V",
        "outputId": "7faa36f7-a0a5-47c0-af88-5cad09fc2199",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name '_name_' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-64e937104266>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0m_name_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"_main_\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Load sample data (this would be real patient data in practice)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;31m# This is just a placeholder - in reality, you would load your own dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name '_name_' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}